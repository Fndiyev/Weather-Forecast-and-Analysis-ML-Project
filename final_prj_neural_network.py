# -*- coding: utf-8 -*-
"""Final prj Neural Network.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/190fEje0kffhZl9TIzxcm7iZpLil4UKgm
"""

import numpy as np, seaborn as sns, pandas as pd, matplotlib.pyplot as plt, matplotlib as mpl
import warnings
warnings.filterwarnings("ignore")
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
mpl.rcParams['figure.figsize'] = [16, 9]
import pickle

import google.colab
google.colab.drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/weather_2.csv')
df

sns.heatmap(df.select_dtypes(exclude='object').corr(), annot=True, cmap='viridis')
plt.show()

df['date'] = pd.to_datetime(df['date'])
df['date'] = df['date'].dt.tz_localize(None)

unique_cities = df['city'].unique()

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import pandas as pd

df['rain_cat'].value_counts()

cols = ['temperature', 'wind_speed', 'wind_direction', 'humidity',
            'precipitation', 'visibility', 'surface_pressure', 'cloud_cover', 'uv_index']
X = df[cols]

# Target: Convert 'rain_cat' to binary (0 = none, 1 = any rain)
y = (df['rain_cat'] != 'none').astype(int)

scaler = StandardScaler()
X = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

device = 'cuda' if torch.cuda.is_available() else 'cpu'
X_train_tensor = torch.from_numpy(X_train).float().to(device)
y_train_tensor = torch.from_numpy(y_train.to_numpy()).float().to(device)
X_test_tensor = torch.from_numpy(X_test).float().to(device)
y_test_tensor = torch.from_numpy(y_test.to_numpy()).float().to(device)

n_none = (y_train == 0).sum()
n_rain = (y_train == 1).sum()
total = len(y_train)
weight_none = total / (2 * n_none)
weight_rain = total / (2 * n_rain)
class_weights = torch.tensor([weight_none, weight_rain]).to(device)

class WeatherDataset(Dataset):
    def __init__(self, x_tensor, y_tensor):
        self.x = x_tensor
        self.y = y_tensor

    def __getitem__(self, index):
        return self.x[index], self.y[index]

    def __len__(self):
        return len(self.x)

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

class WeatherClassifier(nn.Module):
    def __init__(self, dimension):
        super(WeatherClassifier, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(dimension, 32),
            nn.ReLU(),
            nn.Linear(32, 16),
            nn.Tanh(),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.model(x)

dimension = X_train.shape[1]
model = WeatherClassifier(dimension).to(device)
model

optimizer = optim.Adam(model.parameters(), lr=0.01)
loss_fn = nn.BCELoss(reduction='none')
optimizer

losses = []
for i in range(50):
    model.train()
    epoch_loss = 0
    for x_batch, y_batch in train_loader:
        y_batch = y_batch.unsqueeze(1)  # Reshape for BCE loss (same as original)
        yhat = model(x_batch)
        # Apply class weights to loss
        weights = class_weights[1] * y_batch + class_weights[0] * (1 - y_batch)
        loss = loss_fn(yhat, y_batch) * weights
        loss = loss.mean()  # Average loss over batch
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    print(f'Epoch {i+1}: {epoch_loss:.4f}')
    losses.append(epoch_loss)

model.eval()
with torch.no_grad():
    preds = model(X_test_tensor)
    preds = (preds > 0.5).float()  # Convert probabilities to binary predictions
    preds = preds.cpu().numpy()
    y_test_np = y_test_tensor.cpu().numpy()

accuracy = accuracy_score(y_test_np, preds)
precision = precision_score(y_test_np, preds)
recall = recall_score(y_test_np, preds)
f1 = f1_score(y_test_np, preds)

print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')

plt.plot(losses)
plt.title("Loss Plot")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.show()

cm = confusion_matrix(y_test_np, preds)
plt.figure(figsize=(5, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='viridis')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

with open('weather_classifier_model.pkl', 'wb') as f:
    pickle.dump(model.state_dict(), f)

with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

# Convert to PyTorch tensors


# Define dataset


# Create data loader


# Define neural network model (same architecture as heart disease model)


# Initialize model


# Define optimizer and loss function


# Training loop


# Evaluate model



